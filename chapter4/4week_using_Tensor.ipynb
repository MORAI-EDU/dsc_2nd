{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor 생성하기 (P19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\user\\AppData\\Local\\Microsoft\\WindowsApps\\python3.10.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/user/AppData/Local/Microsoft/WindowsApps/python3.10.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch                                                        # PyTorch 라이브러리를 추가\n",
    "\n",
    "print(torch.cuda.is_available())                                    # cuda를 활용한 gpu 연산이 가능한지 확인\n",
    "print(torch.tensor([1,2,3,4,5,6]))                                  # 1차원 벡터 형태의 Tensor 생성\n",
    "print(torch.tensor([[1,2,3],[4,5,6]], device=\"cuda:0\"))             # 2차원 형태의 Tensor를 GPU에 생성\n",
    "print(torch.tensor([[[1,2],[3,4],[5,6]]], dtype=torch.float64))     # 3차원 형태의 Tensor를 float64의 형식으로 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성한 Tensor의 타입 확인하기 (P20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of [1,2,3,4,5,6]\t\t :  torch.LongTensor\n",
      "type of [1.2,2,3,4,5,6]\t\t :  torch.FloatTensor\n",
      "type of [[[1,2],[3,4],[5,6]]]\t :  torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "print(\"type of [1,2,3,4,5,6]\\t\\t : \", torch.tensor([1,2,3,4,5,6]).type())\n",
    "print(\"type of [1.2,2,3,4,5,6]\\t\\t : \", torch.tensor([1.2,2,3,4,5,6]).type())\n",
    "print(\"type of [[[1,2],[3,4],[5,6]]]\\t : \", torch.tensor([[[1,2],[3,4],[5,6]]], dtype=torch.float64).type())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor의 numpy() method를 통한 ndarray 변환(P 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix tensor\t\t :  tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "ndarray translation\t :  [[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "tmp_tensor = torch.tensor([[1,2],[3,4]])\n",
    "\n",
    "print(\"matrix tensor\\t\\t : \",tmp_tensor)\n",
    "arr = tmp_tensor.numpy()\n",
    "\n",
    "print(\"ndarray translation\\t : \", arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix tensor\t\t :  tensor([[1, 2],\n",
      "        [3, 4]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35764\\395731340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"matrix tensor\\t\\t : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtmp_gpu_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0marr_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_gpu_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ndarray translation\\t : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "tmp_gpu_tensor = torch.tensor([[1,2],[3,4]], device=\"cuda:0\")\n",
    "\n",
    "print(\"matrix tensor\\t\\t : \",tmp_gpu_tensor)\n",
    "arr_gpu = tmp_gpu_tensor.numpy()\n",
    "\n",
    "print(\"ndarray translation\\t : \", arr_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix tensor\t\t :  tensor([[1, 2],\n",
      "        [3, 4]], device='cuda:0')\n",
      "ndarray translation\t :  [[1 2]\n",
      " [3 4]]\n",
      "ndarray translation\t :  [[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "tmp_gpu_tensor = torch.tensor([[1,2],[3,4]], device=\"cuda:0\")\n",
    "\n",
    "print(\"matrix tensor\\t\\t : \",tmp_gpu_tensor)\n",
    "arr_gpu_1 = tmp_gpu_tensor.cpu().numpy()\n",
    "arr_gpu_2 = tmp_gpu_tensor.to('cpu').numpy()\n",
    "\n",
    "print(\"ndarray translation\\t : \", arr_gpu_1)\n",
    "print(\"ndarray translation\\t : \", arr_gpu_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor의 인덱싱(P 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print matrix 1 row, 1 col\t :  tensor(5) tensor(5)\n",
      "print matrix 0 row, all col\t :  tensor([1, 2, 3])\n",
      "print matrix all row, 0 col\t :  tensor([1, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_matrix = torch.tensor([[1,2,3],[4,5,6]])\n",
    "\n",
    "print(\"print matrix 1 row, 1 col\\t : \",tensor_matrix[1][1], tensor_matrix[-1][1])\n",
    "print(\"print matrix 0 row, all col\\t : \", tensor_matrix[0,:])\n",
    "print(\"print matrix all row, 0 col\\t : \", tensor_matrix[:,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor의 연산(P 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1 type\t :  torch.FloatTensor\n",
      "tensor2 type\t :  torch.DoubleTensor\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]], dtype=torch.float64)\n",
      "tensor([[2., 6.],\n",
      "        [4., 8.]], dtype=torch.float64)\n",
      "tensor([[2., 6.],\n",
      "        [4., 8.]])\n",
      "tensor([[ 2.5000,  7.5000],\n",
      "        [ 5.0000, 10.0000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 1.,  9.],\n",
      "        [ 4., 16.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.FloatTensor([[1,3],[2,4]])\n",
    "tensor2 = torch.DoubleTensor([[1,3],[2,4]])\n",
    "\n",
    "print(\"tensor1 type\\t : \", tensor1.type())\n",
    "print(\"tensor2 type\\t : \", tensor2.type())\n",
    "\n",
    "sub_tensor = tensor1 - tensor2\n",
    "add_tensor = tensor1 + tensor2\n",
    "mul_int_tensor = tensor1 * 2\n",
    "mul_float_tensor = tensor1 * 2.5\n",
    "divide_tensor = tensor1 / tensor2\n",
    "elmentwise_tensor = tensor1 * tensor2\n",
    "\n",
    "print(sub_tensor)\n",
    "print(add_tensor)\n",
    "print(mul_int_tensor)\n",
    "print(mul_float_tensor)\n",
    "print(divide_tensor)\n",
    "print(elmentwise_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35764\\3337616886.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmultiply_tensor_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmultiply_tensor_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "multiply_tensor_1 = tensor1.matmul(tensor2)\n",
    "\n",
    "print(multiply_tensor_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float_tensor\t :  tensor([[ 7., 15.],\n",
      "        [10., 22.]])\n",
      "float_tensor type\t :  torch.FloatTensor\n",
      "double_tensor\t :  tensor([[ 7., 15.],\n",
      "        [10., 22.]], dtype=torch.float64)\n",
      "double_tensor type\t :  torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "multiply_tensor_2 = tensor1.matmul(tensor2.float())\n",
    "multiply_tensor_3 = tensor1.to(torch.double).matmul(tensor2)\n",
    "\n",
    "print(\"float_tensor\\t : \", multiply_tensor_2)\n",
    "print(\"float_tensor type\\t : \", multiply_tensor_2.type())\n",
    "print(\"double_tensor\\t : \", multiply_tensor_3)\n",
    "print(\"double_tensor type\\t : \", multiply_tensor_3.type())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor의 차원 조작(P 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view(-1) result\t :  tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "view(2,-1) result\t :  tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "view(4,-1) result\t :  tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "view(2,2,2) result\t :  tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "reshape(2,2,2) result\t :  tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7],\n",
      "        [8]])\n"
     ]
    }
   ],
   "source": [
    "tensor_matrix_2 = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "\n",
    "reshape_tensor_1 = tensor_matrix_2.view(-1)\n",
    "reshape_tensor_2 = tensor_matrix_2.view(2,-1)\n",
    "reshape_tensor_3 = tensor_matrix_2.view(4,-1)\n",
    "reshape_tensor_4 = tensor_matrix_2.view(2,2,2)\n",
    "reshape_tensor_5 = tensor_matrix_2.reshape(8,-1)\n",
    "\n",
    "print(\"view(-1) result\\t : \", reshape_tensor_1)\n",
    "print(\"view(2,-1) result\\t : \", reshape_tensor_2)\n",
    "print(\"view(4,-1) result\\t : \", reshape_tensor_3)\n",
    "print(\"view(2,2,2) result\\t : \", reshape_tensor_4)\n",
    "print(\"reshape(2,2,2) result\\t : \", reshape_tensor_5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor의 차원 생성 및 제거(P 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random tensor shape \t\t :  torch.Size([1, 3, 1, 4])\n",
      "squeeze tensor shape \t\t :  torch.Size([3, 4])\n",
      "squeeze 2 idx tensor shape \t :  torch.Size([1, 3, 4])\n",
      "unsqueeze 0 idx \t\t :  torch.Size([1, 3, 4])\n",
      "unsqueeze 1 idx \t\t :  torch.Size([3, 1, 4])\n",
      "unsqueeze 2 idx \t\t :  torch.Size([3, 4, 1])\n",
      "unsqueeze 0, 2 idx \t\t :  torch.Size([1, 3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "random_tensor = torch.randn(1,3,1,4)\n",
    "print(\"random tensor shape \\t\\t : \",random_tensor.shape)\n",
    "squeeze_random1 = random_tensor.squeeze()\n",
    "squeeze_random2 = random_tensor.squeeze(2)\n",
    "\n",
    "\n",
    "print(\"squeeze tensor shape \\t\\t : \",squeeze_random1.shape)\n",
    "print(\"squeeze 2 idx tensor shape \\t : \",squeeze_random2.shape)\n",
    "\n",
    "unsqueeze_random1 = squeeze_random1.unsqueeze(0)\n",
    "unsqueeze_random2 = squeeze_random1.unsqueeze(1)\n",
    "unsqueeze_random3 = squeeze_random1.unsqueeze(2)\n",
    "unsqueeze_random4 = squeeze_random1.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "\n",
    "print(\"unsqueeze 0 idx \\t\\t : \", unsqueeze_random1.shape)\n",
    "print(\"unsqueeze 1 idx \\t\\t : \", unsqueeze_random2.shape)\n",
    "print(\"unsqueeze 2 idx \\t\\t : \", unsqueeze_random3.shape)\n",
    "print(\"unsqueeze 0, 2 idx \\t\\t : \", unsqueeze_random4.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor의 결합-cat(P 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,5) + (4, 5) idx 0 \t\t :  torch.Size([7, 5])\n",
      "(3,5) + (3, 2) idx 0 \t\t :  torch.Size([3, 7])\n",
      "(1, 1, 4) + (5, 1, 4) idx 0 \t :  torch.Size([6, 1, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 3 but got size 4 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16896\\2514826805.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mconcat_tensor5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_tensor1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand_tensor2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_tensor5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 3 but got size 4 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "rand_tensor1 = torch.randn(3, 5)\n",
    "rand_tensor2 = torch.randn(4, 5)\n",
    "rand_tensor3 = torch.randn(3, 2)\n",
    "concat_tensor1 = torch.cat([rand_tensor1, rand_tensor2], dim=0)\n",
    "print(\"(3,5) + (4, 5) idx 0 \\t\\t : \",concat_tensor1.shape)\n",
    "concat_tensor2 = torch.cat([rand_tensor1, rand_tensor3], dim=1)\n",
    "print(\"(3,5) + (3, 2) idx 0 \\t\\t : \",concat_tensor2.shape)\n",
    "\n",
    "rand_tensor4 = torch.randn(1,1,4)\n",
    "rand_tensor5 = torch.randn(5,1,4)\n",
    "\n",
    "concat_tensor4 = torch.cat([rand_tensor4, rand_tensor5], dim=0)\n",
    "print(\"(1, 1, 4) + (5, 1, 4) idx 0 \\t : \",concat_tensor4.shape)\n",
    "\n",
    "\n",
    "concat_tensor5 = torch.cat([rand_tensor1, rand_tensor2], dim=1)\n",
    "print(concat_tensor5.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor의 결합-stack(P 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack (3,5) + (3,5) idx 0 \t :  torch.Size([2, 3, 5])\n",
      "stack (3,5) + (3,5) idx 1 \t :  torch.Size([3, 2, 5])\n",
      "stack (3,5) + (3,5) idx 2 \t :  torch.Size([3, 5, 2])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16896\\2214752181.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stack (3,5) + (3,5) idx 2 \\t : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_stack3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtensor_stack4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_tensor1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand_tensor2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stack (3,5) + (3,5) idx 2 \\t : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_stack4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
     ]
    }
   ],
   "source": [
    "same_size_tensor1 = torch.randn(3, 5)\n",
    "same_size_tensor2 = torch.randn(3, 5)\n",
    "\n",
    "\n",
    "tensor_stack1 = torch.stack([rand_tensor1, rand_tensor2], dim=0)\n",
    "print(\"stack (3,5) + (3,5) idx 0 \\t : \", tensor_stack1.shape)\n",
    "tensor_stack2 = torch.stack([rand_tensor1, rand_tensor2], dim=1)\n",
    "print(\"stack (3,5) + (3,5) idx 1 \\t : \", tensor_stack2.shape)\n",
    "tensor_stack3 = torch.stack([rand_tensor1, rand_tensor2], dim=2)\n",
    "print(\"stack (3,5) + (3,5) idx 2 \\t : \", tensor_stack3.shape)\n",
    "\n",
    "tensor_stack4 = torch.stack([rand_tensor1, rand_tensor2], dim=3)\n",
    "print(\"stack (3,5) + (3,5) idx 2 \\t : \", tensor_stack4.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
